# ğŸ§  Multimodal Facial and Vocal Emotion Recognition  
### Transformer-Based Audioâ€“Visual Emotion Fusion System  

## ğŸ“Œ Overview  

This project presents a multimodal deep learning system for emotion recognition using both facial expressions and speech signals. The system integrates Vision Transformers (ViT) for facial feature extraction, Wav2Vec2 for speech representation learning, and a cross-modal Transformer for audioâ€“visual fusion.

Unlike traditional unimodal approaches, this framework jointly models visual and acoustic cues to improve robustness in real-world conditions such as poor lighting, facial occlusion, and background noise. The implementation supports real emotion datasets including MELD, IEMOCAP, and CMU-MOSEI.

This project is developed as part of an academic research review and experimental framework for multimodal affective computing.

---

## ğŸš€ Key Features  

- Multimodal emotion recognition (face + voice)  
- Vision Transformer (ViT) for facial embeddings  
- Wav2Vec2 for speech emotion embeddings  
- Cross-modal Transformer fusion  
- Real dataset integration (MELD / IEMOCAP / CMU-MOSEI)  
- Real-time prediction from image + audio files  
- Training and evaluation pipeline  
- Edge-aware architecture support  
- Academic review + experimental framework  

---

## ğŸ“Š Emotion Classes  

0 â€“ Neutral
1 â€“ Happy
2 â€“ Sad
3 â€“ Angry
4 â€“ Fear
5 â€“ Surprise
6 â€“ Disgust


---

## ğŸ— System Architecture  

Image Frame â†’ ViT â†’ Visual Embeddings â”
â†’ Cross-Modal Transformer â†’ Emotion Classifier
Audio Signal â†’ Wav2Vec2 â†’ Audio Embeddings â”˜


---

## ğŸ“ Project Structure  

.
â”œâ”€â”€ transformer_fusion_emotion.py
â”œâ”€â”€ datasets/
â”‚ â””â”€â”€ MELD / IEMOCAP / CMU-MOSEI
â”œâ”€â”€ models/
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


---

## âš™ Installation  

### Create virtual environment  

```bash
python -m venv env
source env/bin/activate   # Windows: env\Scripts\activate
Install dependencies
pip install torch torchvision torchaudio transformers timm librosa opencv-python datasets scikit-learn
ğŸ“¥ Dataset Setup
MELD (automatic loading)
from datasets import load_dataset
dataset = load_dataset("declare-lab/MELD")
IEMOCAP / CMU-MOSEI
Download manually:

IEMOCAP: https://sail.usc.edu/iemocap

CMU-MOSEI: https://github.com/A2Zadeh/CMU-MultimodalSDK

Place datasets inside:

datasets/
â–¶ Running the Model
Real Emotion Prediction
predict_from_files("face.jpg", "audio.wav")
Outputs:

Predicted emotion

Confidence score

Probability distribution

Training
python transformer_fusion_emotion.py
Default configuration:

Epochs: 20

Optimizer: AdamW

Loss: Label-smoothed Cross Entropy

First 5 epochs: frozen ViT + Wav2Vec2

ğŸ“ˆ Example Output
Predicted Emotion: Happy
Confidence: 0.87

Class Probabilities:
Neutral: 0.02
Happy: 0.87
Sad: 0.04
Angry: 0.03
Fear: 0.01
Surprise: 0.02
Disgust: 0.01
ğŸ§ª Training Strategy
Pretrained ViT + Wav2Vec2 frozen initially

Cross-modal Transformer trained first

Gradual unfreezing for fine-tuning

Feature-level fusion

Attention-based modality weighting

ğŸ”¬ Research Motivation
Human emotion perception is inherently multimodal. Facial expressions alone or speech alone are insufficient in real-world environments. This project demonstrates how joint audioâ€“visual learning significantly improves emotion recognition robustness.

âš  Ethical Considerations
Emotion recognition involves sensitive biometric data

On-device inference is encouraged

No identity storage

Intended strictly for academic research

ğŸ‘¨â€ğŸ“ Authors
Saaransh Malik
Arnav Juneja

Manipal University Jaipur
Department of Computer Science Engineering
