<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Emotion Recognition Datasets and Resources</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background: #f4f6f8;
            margin: 0;
            padding: 0;
        }
        header {
            background: #0b3d91;
            color: white;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 900px;
            margin: 30px auto;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h2 {
            color: #0b3d91;
        }
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin: 12px 0;
            padding: 10px;
            background: #eef3ff;
            border-radius: 6px;
        }
        a {
            text-decoration: none;
            color: #0b3d91;
            font-weight: bold;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            text-align: center;
            padding: 15px;
            color: gray;
        }
    </style>
</head>
<body>

<header>
    <h1>Emotion Recognition Datasets & Resources</h1>
    <p>Multimodal Emotion Recognition: Facial, Speech, and Multimodal Datasets</p>
</header>

<div class="container">

    <h2>Multimodal Emotion Datasets</h2>
    <ul>
        <li>
            CMU-MOSEI Dataset — 
            <a href="https://github.com/A2Zadeh/CMU-MultimodalSDK" target="_blank">
                https://github.com/A2Zadeh/CMU-MultimodalSDK
            </a>
        </li>
        <li>
            MELD Dataset — 
            <a href="https://affective-meld.github.io/" target="_blank">
                https://affective-meld.github.io/
            </a>
        </li>
        <li>
            IEMOCAP Dataset — 
            <a href="https://sail.usc.edu/iemocap/" target="_blank">
                https://sail.usc.edu/iemocap/
            </a>
        </li>
        <li>
            RAVDESS Dataset — 
            <a href="https://zenodo.org/record/1188976" target="_blank">
                https://zenodo.org/record/1188976
            </a>
        </li>
    </ul>

    <h2>Facial Expression Datasets</h2>
    <ul>
        <li>
            AffectNet Dataset — 
            <a href="http://mohammadmahoor.com/affectnet/" target="_blank">
                http://mohammadmahoor.com/affectnet/
            </a>
        </li>
    </ul>

    <h2>Models and Frameworks</h2>
    <ul>
        <li>
            Wav2Vec 2.0 — 
            <a href="https://huggingface.co/facebook/wav2vec2-base" target="_blank">
                https://huggingface.co/facebook/wav2vec2-base
            </a>
        </li>
        <li>
            FaceNet — 
            <a href="https://arxiv.org/abs/1503.03832" target="_blank">
                https://arxiv.org/abs/1503.03832
            </a>
        </li>
    </ul>

    <h2>Emotion Coding Standards</h2>
    <ul>
        <li>
            Facial Action Coding System (FACS) — 
            <a href="https://www.paulekman.com/facial-action-coding-system/" target="_blank">
                https://www.paulekman.com/facial-action-coding-system/
            </a>
        </li>
    </ul>

</div>

<footer>
    Created for Multimodal Emotion Recognition Research Project
</footer>

</body>
</html>